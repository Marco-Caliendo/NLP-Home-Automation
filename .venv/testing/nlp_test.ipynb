{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:47:04.747930Z",
     "start_time": "2024-10-01T22:47:04.732934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "#Key Changes For Faster Training:\n",
    "#Batching and Padding: prepare_batch prepares input and target batches with padding and corresponding sequence lengths.\n",
    "#Packed Sequences: The forward method handles packed sequences to avoid unnecessary computation on padded tokens.\n",
    "#Gradient Clipping: Applied gradient clipping to avoid exploding gradients.\n",
    "#Mixed Precision Training: Using torch.cuda.amp with automatic casting and gradient scaling to accelerate training on GPUs with mixed precision.\n",
    "#Reduced Hidden Size: Reduced the hidden size to 64 for faster training.\n",
    "#Ignored Padding in Loss: ignore_index=vocab['<PAD>'] ensures padding tokens are ignored during loss calculation\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Chatbot Model Definition\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state, lengths):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, hidden_state = self.lstm(packed_embedded, hidden_state)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "# Tokenize and preprocess text data\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "# Build a vocabulary from the dataset\n",
    "def build_vocab(dataset):\n",
    "    all_words = []\n",
    "    for sentence in dataset:\n",
    "        all_words.extend(tokenize(sentence))\n",
    "\n",
    "    vocab = {word: i for i, word in enumerate(set(all_words))}\n",
    "    vocab['<PAD>'] = len(vocab)  # Padding token\n",
    "    vocab['<SOS>'] = len(vocab)  # Start of sentence\n",
    "    vocab['<EOS>'] = len(vocab)  # End of sentence\n",
    "    return vocab\n",
    "\n",
    "# Prepare batch with padding\n",
    "def prepare_batch(input_tensors, target_tensors, batch_size):\n",
    "    idx = np.random.choice(len(input_tensors), batch_size, replace=False)\n",
    "    batch_inputs = [input_tensors[i] for i in idx]\n",
    "    batch_targets = [target_tensors[i] for i in idx]\n",
    "\n",
    "    # Pad the input and target sequences to the same length\n",
    "    padded_inputs = pad_sequence(batch_inputs, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "    padded_targets = pad_sequence(batch_targets, batch_first=True, padding_value=vocab['<PAD>'])\n",
    "    lengths = [len(tensor) for tensor in batch_inputs]\n",
    "\n",
    "    return padded_inputs, padded_targets, lengths\n",
    "\n",
    "# Training function\n",
    "def train(chatbot_model, input_tensor, target_tensor, optimizer, criterion, batch_size, lengths, scaler):\n",
    "    hidden_state = chatbot_model.init_hidden(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "\n",
    "    with autocast():\n",
    "        output, hidden_state = chatbot_model(input_tensor, hidden_state, lengths)\n",
    "\n",
    "        output = output.view(-1, output.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "        target_tensor = target_tensor.view(-1)     # (batch_size * seq_len)\n",
    "\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(chatbot_model.parameters(), max_norm=2.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Generate response function\n",
    "def generate_response(chatbot_model, input_seq, vocab, max_len=20):\n",
    "    chatbot_model.eval()\n",
    "\n",
    "    input_tensor = torch.tensor([vocab[word] for word in tokenize(input_seq)], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden_state = chatbot_model.init_hidden(1)  # Batch size of 1 for inference\n",
    "\n",
    "    response = []\n",
    "    for _ in range(max_len):\n",
    "        output, hidden_state = chatbot_model(input_tensor, hidden_state, [len(input_tensor[0])])\n",
    "\n",
    "        _, top_word = torch.max(output[:, -1, :], dim=1)\n",
    "\n",
    "        word = list(vocab.keys())[list(vocab.values()).index(top_word.item())]\n",
    "        response.append(word)\n",
    "\n",
    "        if word == '<EOS>':\n",
    "            break\n",
    "\n",
    "        input_tensor = torch.tensor([[top_word.item()]], dtype=torch.long).to(device)\n",
    "\n",
    "    return ' '.join(response[:-1])  # Remove <EOS> from the response"
   ],
   "id": "1a0a1880c34ce8d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:49:26.418775Z",
     "start_time": "2024-10-01T22:49:25.821743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example dataset\n",
    "with open('moby_dick.txt', 'r', encoding='utf-8') as f:\n",
    "    dataset = f.readlines()\n",
    "print(dataset[:10])\n",
    "\n",
    "# Build vocab and tokenize the data\n",
    "vocab = build_vocab(dataset)\n",
    "\n",
    "# Convert dataset to tensors\n",
    "input_tensors = []\n",
    "target_tensors = []\n",
    "for sentence in dataset:\n",
    "    tokenized = tokenize(sentence)\n",
    "    input_tensors.append(torch.tensor([vocab['<SOS>']] + [vocab[word] for word in tokenized], dtype=torch.long))\n",
    "    target_tensors.append(torch.tensor([vocab[word] for word in tokenized] + [vocab['<EOS>']], dtype=torch.long))"
   ],
   "id": "f543238c729da0b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff  ETYMOLOGY.\\n', '\\n', '\\n', '  (Supplied by a Late Consumptive Usher to a Grammar School.)\\n', '\\n', '  The pale Usherâ€”threadbare in coat, heart, body, and brain; I see him\\n', '  now. He was ever dusting his old lexicons and grammars, with a queer\\n', '  handkerchief, mockingly embellished with all the gay flags of all the\\n', '  known nations of the world. He loved to dust his old grammars; it\\n', '  somehow mildly reminded him of his mortality.\\n']\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T03:21:51.601169Z",
     "start_time": "2024-10-02T03:21:51.554425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameters\n",
    "input_size = len(vocab)\n",
    "hidden_size = 128\n",
    "output_size = len(vocab)\n",
    "n_layers = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "chatbot_model = ChatbotModel(input_size, hidden_size, output_size, n_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>']).to(device)\n",
    "optimizer = optim.Adam(chatbot_model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()"
   ],
   "id": "222c06119bd37905",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calie\\AppData\\Local\\Temp\\ipykernel_18880\\2700925018.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T03:44:23.254856Z",
     "start_time": "2024-10-02T03:21:52.898520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    chatbot_model.train()\n",
    "\n",
    "    for _ in range(len(input_tensors) // batch_size):\n",
    "        batch_input, batch_target, lengths = prepare_batch(input_tensors, target_tensors, batch_size)\n",
    "        loss = train(chatbot_model, batch_input, batch_target, optimizer, criterion, batch_size, lengths, scaler)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {total_loss / (len(input_tensors) // batch_size)}')"
   ],
   "id": "76b19b22e519df68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calie\\AppData\\Local\\Temp\\ipykernel_18880\\722915003.py:94: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.9228547668457034\n",
      "Epoch 1, Loss: 6.349125294155544\n",
      "Epoch 2, Loss: 6.017546099203604\n",
      "Epoch 3, Loss: 5.778509854917173\n",
      "Epoch 4, Loss: 5.615060405731201\n",
      "Epoch 5, Loss: 5.462915462917752\n",
      "Epoch 6, Loss: 5.287902078275327\n",
      "Epoch 7, Loss: 5.166946064277932\n",
      "Epoch 8, Loss: 5.073582969948098\n",
      "Epoch 9, Loss: 4.932626134377939\n",
      "Epoch 10, Loss: 4.858662592569987\n",
      "Epoch 11, Loss: 4.7833669330455635\n",
      "Epoch 12, Loss: 4.702167423389576\n",
      "Epoch 13, Loss: 4.643877285851373\n",
      "Epoch 14, Loss: 4.64052464167277\n",
      "Epoch 15, Loss: 4.582711551101119\n",
      "Epoch 16, Loss: 4.526378768638328\n",
      "Epoch 17, Loss: 4.443266615691009\n",
      "Epoch 18, Loss: 4.3906854693094886\n",
      "Epoch 19, Loss: 4.338305949105157\n",
      "Epoch 20, Loss: 4.216825308269924\n",
      "Epoch 21, Loss: 4.219532985687255\n",
      "Epoch 22, Loss: 4.257833579028095\n",
      "Epoch 23, Loss: 4.239511962113557\n",
      "Epoch 24, Loss: 4.153317638326574\n",
      "Epoch 25, Loss: 4.059692322059914\n",
      "Epoch 26, Loss: 3.998166763164379\n",
      "Epoch 27, Loss: 3.943091046368634\n",
      "Epoch 28, Loss: 3.8598095879731353\n",
      "Epoch 29, Loss: 3.878744265238444\n",
      "Epoch 30, Loss: 3.9768011096671776\n",
      "Epoch 31, Loss: 3.8914423712977655\n",
      "Epoch 32, Loss: 3.7627526982625326\n",
      "Epoch 33, Loss: 3.633605764883536\n",
      "Epoch 34, Loss: 3.690993198112205\n",
      "Epoch 35, Loss: 3.6203514487655073\n",
      "Epoch 36, Loss: 3.512395161875972\n",
      "Epoch 37, Loss: 3.3319551838768855\n",
      "Epoch 38, Loss: 3.2563024482020624\n",
      "Epoch 39, Loss: 3.4272199736701117\n",
      "Epoch 40, Loss: 3.4410408917179813\n",
      "Epoch 41, Loss: 3.383458303522181\n",
      "Epoch 42, Loss: 3.232428605468185\n",
      "Epoch 43, Loss: 3.3030331936588992\n",
      "Epoch 44, Loss: 3.468805228692514\n",
      "Epoch 45, Loss: 3.5755539894104005\n",
      "Epoch 46, Loss: 3.4836606032760056\n",
      "Epoch 47, Loss: 3.3423583761850995\n",
      "Epoch 48, Loss: 3.156273761325412\n",
      "Epoch 49, Loss: 3.114139816142895\n",
      "Epoch 50, Loss: 3.0132774522569443\n",
      "Epoch 51, Loss: 3.005259906274301\n",
      "Epoch 52, Loss: 3.0699889557449906\n",
      "Epoch 53, Loss: 3.0494151903081823\n",
      "Epoch 54, Loss: 3.0237182595994736\n",
      "Epoch 55, Loss: 3.1191747202696622\n",
      "Epoch 56, Loss: 3.322457775186609\n",
      "Epoch 57, Loss: 3.2647448034639712\n",
      "Epoch 58, Loss: 3.1640158176422117\n",
      "Epoch 59, Loss: 2.9952743279492413\n",
      "Epoch 60, Loss: 3.1734805280190925\n",
      "Epoch 61, Loss: 3.1759686915079754\n",
      "Epoch 62, Loss: 3.164023741969356\n",
      "Epoch 63, Loss: 3.130167441897922\n",
      "Epoch 64, Loss: 3.114941577911377\n",
      "Epoch 65, Loss: 3.0890535796130143\n",
      "Epoch 66, Loss: 3.035986172004982\n",
      "Epoch 67, Loss: 3.0726157545160366\n",
      "Epoch 68, Loss: 3.0682061562714753\n",
      "Epoch 69, Loss: 2.955389216740926\n",
      "Epoch 70, Loss: 2.834568678184792\n",
      "Epoch 71, Loss: 2.8164327261182995\n",
      "Epoch 72, Loss: 3.0423336304558646\n",
      "Epoch 73, Loss: 3.0497332046650074\n",
      "Epoch 74, Loss: 3.0256531994431106\n",
      "Epoch 75, Loss: 2.854426277655142\n",
      "Epoch 76, Loss: 2.7995629045698376\n",
      "Epoch 77, Loss: 3.0007179578145347\n",
      "Epoch 78, Loss: 3.028509779682866\n",
      "Epoch 79, Loss: 3.002314464074594\n",
      "Epoch 80, Loss: 2.8287252563900416\n",
      "Epoch 81, Loss: 2.7769780572255454\n",
      "Epoch 82, Loss: 2.8481715262377705\n",
      "Epoch 83, Loss: 2.995114478358516\n",
      "Epoch 84, Loss: 2.9849396772737857\n",
      "Epoch 85, Loss: 2.8998162075325293\n",
      "Epoch 86, Loss: 2.93765792493467\n",
      "Epoch 87, Loss: 2.958856363296509\n",
      "Epoch 88, Loss: 2.94341135625486\n",
      "Epoch 89, Loss: 2.9109518863536694\n",
      "Epoch 90, Loss: 2.93402949474476\n",
      "Epoch 91, Loss: 2.9096442540486653\n",
      "Epoch 92, Loss: 2.8062422999629266\n",
      "Epoch 93, Loss: 2.725597839002256\n",
      "Epoch 94, Loss: 2.6863889952059146\n",
      "Epoch 95, Loss: 2.668676540939896\n",
      "Epoch 96, Loss: 2.926772569373802\n",
      "Epoch 97, Loss: 2.9425976520114476\n",
      "Epoch 98, Loss: 2.925490469402737\n",
      "Epoch 99, Loss: 2.815588906606038\n",
      "Epoch 100, Loss: 2.9121609140325475\n",
      "Epoch 101, Loss: 2.890455000842059\n",
      "Epoch 102, Loss: 2.854787273760195\n",
      "Epoch 103, Loss: 2.770732002964726\n",
      "Epoch 104, Loss: 2.8853122838338217\n",
      "Epoch 105, Loss: 2.892198682007966\n",
      "Epoch 106, Loss: 2.8384557999504936\n",
      "Epoch 107, Loss: 2.8462712284370704\n",
      "Epoch 108, Loss: 2.8768009874555798\n",
      "Epoch 109, Loss: 2.869606941011217\n",
      "Epoch 110, Loss: 2.7695400283954763\n",
      "Epoch 111, Loss: 2.772960329055786\n",
      "Epoch 112, Loss: 3.1445392174190947\n",
      "Epoch 113, Loss: 3.1283640455316615\n",
      "Epoch 114, Loss: 3.0964271389996565\n",
      "Epoch 115, Loss: 3.054674480226305\n",
      "Epoch 116, Loss: 3.0468209768224646\n",
      "Epoch 117, Loss: 3.0412620272459807\n",
      "Epoch 118, Loss: 2.8885690968124953\n",
      "Epoch 119, Loss: 2.826416248038963\n",
      "Epoch 120, Loss: 2.820119441703514\n",
      "Epoch 121, Loss: 2.70155088212755\n",
      "Epoch 122, Loss: 2.675042719664397\n",
      "Epoch 123, Loss: 2.69895397928026\n",
      "Epoch 124, Loss: 2.8703686837796814\n",
      "Epoch 125, Loss: 2.8950113427197492\n",
      "Epoch 126, Loss: 2.884623942198577\n",
      "Epoch 127, Loss: 2.8608700455559624\n",
      "Epoch 128, Loss: 2.8784787358178034\n",
      "Epoch 129, Loss: 2.8475782277848984\n",
      "Epoch 130, Loss: 2.716579811308119\n",
      "Epoch 131, Loss: 2.687584508260091\n",
      "Epoch 132, Loss: 2.6547212802039253\n",
      "Epoch 133, Loss: 2.765285656540482\n",
      "Epoch 134, Loss: 2.989694425088388\n",
      "Epoch 135, Loss: 3.1271039644877114\n",
      "Epoch 136, Loss: 3.114706360499064\n",
      "Epoch 137, Loss: 3.019413914150662\n",
      "Epoch 138, Loss: 2.8904914654625786\n",
      "Epoch 139, Loss: 2.8438122431437174\n",
      "Epoch 140, Loss: 2.8076216983795166\n",
      "Epoch 141, Loss: 2.7056349545938\n",
      "Epoch 142, Loss: 2.770368163144147\n",
      "Epoch 143, Loss: 2.8724814072361697\n",
      "Epoch 144, Loss: 2.896123319908425\n",
      "Epoch 145, Loss: 2.863559240411829\n",
      "Epoch 146, Loss: 2.8878651173909504\n",
      "Epoch 147, Loss: 2.8849085765414766\n",
      "Epoch 148, Loss: 2.8126821115281846\n",
      "Epoch 149, Loss: 2.721093612600256\n",
      "Epoch 150, Loss: 2.6979052529511627\n",
      "Epoch 151, Loss: 2.92235701419689\n",
      "Epoch 152, Loss: 3.1217109775543213\n",
      "Epoch 153, Loss: 3.0876989361091898\n",
      "Epoch 154, Loss: 3.0011223037154586\n",
      "Epoch 155, Loss: 2.885022296199092\n",
      "Epoch 156, Loss: 2.8428873846266005\n",
      "Epoch 157, Loss: 2.788276306081701\n",
      "Epoch 158, Loss: 2.721985777395743\n",
      "Epoch 159, Loss: 2.7015367331328215\n",
      "Epoch 160, Loss: 2.796513736512926\n",
      "Epoch 161, Loss: 2.8959654906943992\n",
      "Epoch 162, Loss: 2.8880117907347502\n",
      "Epoch 163, Loss: 2.808026724214907\n",
      "Epoch 164, Loss: 2.943998217759309\n",
      "Epoch 165, Loss: 3.1555081169693557\n",
      "Epoch 166, Loss: 3.1221210108862985\n",
      "Epoch 167, Loss: 3.0196485522941305\n",
      "Epoch 168, Loss: 2.906038661179719\n",
      "Epoch 169, Loss: 2.8606772157880993\n",
      "Epoch 170, Loss: 2.8104480577398228\n",
      "Epoch 171, Loss: 2.7389497092918114\n",
      "Epoch 172, Loss: 2.7680782438207556\n",
      "Epoch 173, Loss: 2.8772506314736828\n",
      "Epoch 174, Loss: 2.882449973071063\n",
      "Epoch 175, Loss: 2.854364044048168\n",
      "Epoch 176, Loss: 2.8018044810824922\n",
      "Epoch 177, Loss: 2.896723386976454\n",
      "Epoch 178, Loss: 2.8965957260131834\n",
      "Epoch 179, Loss: 2.8946784860116463\n",
      "Epoch 180, Loss: 2.889973037507799\n",
      "Epoch 181, Loss: 3.012217951880561\n",
      "Epoch 182, Loss: 3.0821902052561443\n",
      "Epoch 183, Loss: 3.0559420839945477\n",
      "Epoch 184, Loss: 2.956623417183205\n",
      "Epoch 185, Loss: 2.8615920667295103\n",
      "Epoch 186, Loss: 2.8750170442793106\n",
      "Epoch 187, Loss: 3.0287099047060364\n",
      "Epoch 188, Loss: 3.037864390125981\n",
      "Epoch 189, Loss: 3.0030489070327193\n",
      "Epoch 190, Loss: 2.8893049130616366\n",
      "Epoch 191, Loss: 3.0032252403541846\n",
      "Epoch 192, Loss: 3.0415624992935744\n",
      "Epoch 193, Loss: 3.029138451328984\n",
      "Epoch 194, Loss: 2.9045436233944364\n",
      "Epoch 195, Loss: 2.861081662001433\n",
      "Epoch 196, Loss: 2.8433212742982086\n",
      "Epoch 197, Loss: 2.8107820274211743\n",
      "Epoch 198, Loss: 2.8713924439748126\n",
      "Epoch 199, Loss: 2.8589643722110325\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T03:48:07.629872Z",
     "start_time": "2024-10-02T03:48:07.561994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate a response\n",
    "user_input = (\"how are you\")\n",
    "response = generate_response(chatbot_model, user_input, vocab)\n",
    "print(\"Bot:\", response)"
   ],
   "id": "2d8aaeac36ab9a8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: get to get out of the devil many other are\n"
     ]
    }
   ],
   "execution_count": 114
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
